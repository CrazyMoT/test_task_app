Connect with me
https://t.me/motozavr


# Тестовое задание: Распределённая система для обработки и анализа данных о продажах

## Описание
Создать распределённую систему для обработки и анализа данных о продажах. Система должна состоять из микросервисов, каждый в своём контейнере, которые выполняют разные функции: от сбора данных до визуализации и мониторинга.

## Требования
- **Python** – для написания микросервисов.
- **Docker Compose** – для контейнеризации.
- **PostgreSQL** – для хранения данных о продажах и аналитики.
- **Kafka** – для передачи сообщений между микросервисами.
- **Prometheus и Grafana** – для мониторинга производительности микросервисов и визуализации метрик.

## Логика работы
### 1. Data Collector (Сбор данных)
- Принимает данные о продажах в формате JSON через REST API.
- Отправляет сообщения в Kafka.


### 2. Data Processor (Обработка данных)
- Получает данные из Kafka.
- Проводит первичную обработку (валидация, вычисление метрик).
- Записывает обработанные данные в PostgreSQL.

### 3. Analytics Service (Аналитика)
- Извлекает данные из PostgreSQL.
- Генерирует аналитические отчёты (суммарные продажи за день, продажи по продуктам).
- Предоставляет REST API для запросов.

### 4. Notification Service (Уведомления)
- Получает данные из Kafka.
- Отправляет уведомления, если выполняется определённое условие (например, продажи упали ниже X).

### 5. Мониторинг и визуализация (Prometheus + Grafana)
#### Метрики:
- Количество сообщений в Kafka.
- Время обработки сообщений каждым сервисом.
- Использование ресурсов (CPU, память).

#### Дашборды в Grafana:
- Текущие продажи по продуктам.
- Производительность микросервисов.
- Общие тренды (ежедневный рост/спад продаж).

## Инфраструктура (Контейнеры в Docker Compose)
1. **PostgreSQL** – контейнер для базы данных.
2. **Kafka** – контейнер для брокера сообщений.
3. **Python-сервисы (каждый в своём контейнере):**
   - **Data Collector Service** – приём данных о транзакциях через REST API и их отправка в Kafka.
   - **Data Processor Service** – получение данных из Kafka, их валидация и запись в базу PostgreSQL.
   - **Analytics Service** – генерация аналитических данных и предоставление доступа через REST API.
   - **Notification Service** – отправка уведомлений о важных событиях (например, падение продаж ниже X).
4. **Prometheus** – контейнер для сбора метрик.
5. **Grafana** – контейнер для визуализации метрик.

## Примечания
- Логи микросервисов сохранять в **Docker Volumes**.
- Использовать **.env файлы** для конфигурации (подключение к PostgreSQL и Kafka без хардкодинга в коде).
- Вся работа с базой данных должна быть реализована через миграции **(Alembic)**.

## Сущности
### 1. Product (Продукт)
- `product_id` (уникальный идентификатор)
- `name` (название)
- `category` (категория)
- `price` (цена)

### 2. Transaction (Транзакция)
- `transaction_id` (уникальный идентификатор)
- `product_id` (идентификатор продукта)
- `quantity` (количество)
- `total_amount` (общая сумма)
- `timestamp` (время покупки)

### 3. Analytics (Аналитика)
- Дневные/месячные/годовые агрегаты данных по продажам.
- Суммарные продажи по продуктам или категориям.
- Средняя стоимость покупки.

## Kafka топики
- **transactions** – обработка транзакций.
- **notifications** – события для системы уведомлений.

